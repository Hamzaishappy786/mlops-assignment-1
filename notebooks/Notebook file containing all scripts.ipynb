{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2bc632-3a53-4720-bb98-910ee3a6bf5d",
   "metadata": {},
   "source": [
    "## Training of KNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cb46c-3345-4153-8941-341b84985918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Absolute project root\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "mlruns_dir = PROJECT_ROOT / \"mlruns_individual\"\n",
    "\n",
    "# Force MLflow logs into top-level mlruns_individual/\n",
    "mlflow.set_tracking_uri(f\"file:///{mlruns_dir}\")\n",
    "mlflow.set_experiment(\"iris_models_individual\")\n",
    "\n",
    "# Use top-level models/ and results/\n",
    "models_dir = PROJECT_ROOT / \"models\"\n",
    "results_dir = PROJECT_ROOT / \"results\"\n",
    "\n",
    "if not models_dir.exists():\n",
    "    raise FileNotFoundError(f\"Expected 'models/' folder at {models_dir}, but not found.\")\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Expected 'results/' folder at {results_dir}, but not found.\")\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.95, random_state=42\n",
    ")\n",
    "\n",
    "# Train and log with MLflow\n",
    "with mlflow.start_run(run_name=\"KNN_95_5\"):\n",
    "    # Train model (default k=5)\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    rec = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params({\"n_neighbors\": 5, \"test_size\": 0.95, \"train_size\": 0.05})\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "\n",
    "    # Save confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "    plt.title(\"Confusion Matrix - KNN (95% test)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    cm_path = results_dir / \"knn_confusion_95.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Log artifacts\n",
    "    mlflow.log_artifact(str(cm_path))\n",
    "\n",
    "    # Log model\n",
    "    input_example = X_test[:5]\n",
    "    signature = infer_signature(X_train[:10], clf.predict(X_train[:10]))\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        name=\"KNN\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    # Save metrics JSON\n",
    "    metrics_path = results_dir / \"knn_metrics_95.json\"\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1_score\": f1, \"report\": report}, f, indent=4)\n",
    "\n",
    "    # Save model\n",
    "    model_path = models_dir / \"knn_model_95.pkl\"\n",
    "    joblib.dump(clf, model_path)\n",
    "\n",
    "    print(f\"[KNN] Done ‚Üí Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    print(f\"Metrics saved at: {metrics_path}\")\n",
    "    # knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ba27f-5dcf-43a3-afe2-5282172f4495",
   "metadata": {},
   "source": [
    "## Training of SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61813d0-be05-4b8b-8175-c7faac6cfe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Absolute project root\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "mlruns_dir = PROJECT_ROOT / \"mlruns_individual\"\n",
    "\n",
    "# Force MLflow logs into top-level mlruns_individual/\n",
    "mlflow.set_tracking_uri(f\"file:///{mlruns_dir}\")\n",
    "mlflow.set_experiment(\"iris_models_individual\")\n",
    "\n",
    "# Use top-level models/ and results/\n",
    "models_dir = PROJECT_ROOT / \"models\"\n",
    "results_dir = PROJECT_ROOT / \"results\"\n",
    "\n",
    "if not models_dir.exists():\n",
    "    raise FileNotFoundError(f\"Expected 'models/' folder at {models_dir}, but not found.\")\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Expected 'results/' folder at {results_dir}, but not found.\")\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.95, random_state=42\n",
    ")\n",
    "\n",
    "# Train and log with MLflow\n",
    "with mlflow.start_run(run_name=\"SVM_95_5\"):\n",
    "    # Train model (linear kernel keeps size small)\n",
    "    clf = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    rec = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params({\"kernel\": \"linear\", \"probability\": True, \"test_size\": 0.95, \"train_size\": 0.05})\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "\n",
    "    # Save confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Oranges\")\n",
    "    plt.title(\"Confusion Matrix - SVM (95% test)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    cm_path = results_dir / \"svm_confusion_95.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Log artifacts\n",
    "    mlflow.log_artifact(str(cm_path))\n",
    "\n",
    "    # Log model\n",
    "    input_example = X_test[:5]\n",
    "    signature = infer_signature(X_train[:10], clf.predict(X_train[:10]))\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        name=\"SVM\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    # Save metrics JSON\n",
    "    metrics_path = results_dir / \"svm_metrics_95.json\"\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1_score\": f1, \"report\": report}, f, indent=4)\n",
    "\n",
    "    # Save model\n",
    "    model_path = models_dir / \"svm_model_95.pkl\"\n",
    "    joblib.dump(clf, model_path)\n",
    "\n",
    "    print(f\"[SVM] Done ‚Üí Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    print(f\"Metrics saved at: {metrics_path}\")\n",
    "\n",
    "    # svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4ded6-c52c-4a5d-bf0a-fc755c357ade",
   "metadata": {},
   "source": [
    "## Training of Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218e7ac-a4c5-45fc-9b39-d738db2c6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Absolute project root\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "mlruns_dir = PROJECT_ROOT / \"mlruns_individual\"\n",
    "\n",
    "# Force MLflow logs into top-level mlruns_individual/\n",
    "mlflow.set_tracking_uri(f\"file:///{mlruns_dir}\")\n",
    "mlflow.set_experiment(\"iris_models_individual\")\n",
    "\n",
    "# Use top-level models/ and results/\n",
    "models_dir = PROJECT_ROOT / \"models\"\n",
    "results_dir = PROJECT_ROOT / \"results\"\n",
    "\n",
    "# Ensure dirs exist (don‚Äôt recreate inside src)\n",
    "if not models_dir.exists():\n",
    "    raise FileNotFoundError(f\"Expected 'models/' folder at {models_dir}, but not found.\")\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Expected 'results/' folder at {results_dir}, but not found.\")\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.95, random_state=42\n",
    ")\n",
    "\n",
    "# Train and log with MLflow\n",
    "with mlflow.start_run(run_name=\"Logistic_Regression_95_5\"):\n",
    "    # Train model\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    rec = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params({\"max_iter\": 200, \"test_size\": 0.95, \"train_size\": 0.05})\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "\n",
    "    # Save confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix - Logistic Regression (95% test)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    cm_path = results_dir / \"logreg_confusion_95.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Log artifacts\n",
    "    mlflow.log_artifact(str(cm_path))\n",
    "\n",
    "    # Log model\n",
    "    input_example = X_test[:5]\n",
    "    signature = infer_signature(X_train[:10], clf.predict(X_train[:10]))\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        name=\"logistic regression\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    # Save metrics JSON\n",
    "    metrics_path = results_dir / \"logreg_metrics_95.json\"\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1_score\": f1, \"report\": report}, f, indent=4)\n",
    "\n",
    "    # Save model\n",
    "    model_path = models_dir / \"logreg_model_95.pkl\"\n",
    "    joblib.dump(clf, model_path)\n",
    "\n",
    "    print(f\"[LogReg] Done ‚Üí Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"Model saved at: {model_path}\")\n",
    "    print(f\"Metrics saved at: {metrics_path}\")\n",
    "\n",
    "    #log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667aa35e-ebae-456f-9135-9eb6aacf776b",
   "metadata": {},
   "source": [
    "### For checking results on UI():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9b1b6-eac1-4af6-a55d-ee1542aef6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Get absolute path to avoid issues\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "project_root = os.path.dirname(current_dir)\n",
    "mlruns_path = os.path.join(project_root, \"mlruns_individual\")\n",
    "results_path = os.path.join(project_root, \"results\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"MLruns path: {mlruns_path}\")\n",
    "\n",
    "# üìÇ Ensure results dir exists\n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "# üìä Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def evaluate_and_log(model, params, model_name):\n",
    "    \"\"\"Train, evaluate, and log model + metrics + confusion matrix to MLflow.\"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        rec = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "        # Log params + metrics\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1_score\": f1\n",
    "        })\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(f\"{model_name} Confusion Matrix\")\n",
    "        cm_path = os.path.join(results_path, f\"{model_name}_confusion_matrix.png\")\n",
    "        plt.savefig(cm_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Log artifacts (plots + model)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "\n",
    "        # Log model with signature and input example\n",
    "        input_example = X_test[:5]\n",
    "        signature = infer_signature(X_train[:10], model.predict(X_train[:10]))\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            name=\"model\",\n",
    "            input_example=input_example,\n",
    "            signature=signature,\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ {model_name} logged with Accuracy={acc:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# ‚ö° Define Models\n",
    "models = {\n",
    "    \"Logistic_Regression\": (LogisticRegression(max_iter=200), {\"max_iter\": 200}),\n",
    "    \"SVM\": (SVC(kernel=\"linear\", C=1), {\"kernel\": \"linear\", \"C\": 1}),\n",
    "    \"KNN\": (KNeighborsClassifier(n_neighbors=3), {\"n_neighbors\": 3})\n",
    "}\n",
    "\n",
    "# Set tracking URI with absolute path\n",
    "mlflow.set_tracking_uri(f\"file:///{mlruns_path.replace(os.sep, '/')}\")\n",
    "mlflow.set_experiment(\"mlruns_individual\")\n",
    "\n",
    "# üöÄ Train + Log each model\n",
    "for model_name, (model, params) in models.items(): \n",
    "    evaluate_and_log(model, params, model_name)\n",
    "\n",
    "print(f\"\\nüìä All models logged to: {mlruns_path}\")\n",
    "print(f\"üåê To view UI: mlflow ui --backend-store-uri \\\"file:///{mlruns_path.replace(os.sep, '/')}\\\" --host 127.0.0.1 --port 5000\")\n",
    "\n",
    "# Check if files were created\n",
    "if os.path.exists(mlruns_path):\n",
    "    print(f\"‚úÖ MLruns directory created at: {mlruns_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå MLruns directory NOT created at: {mlruns_path}\")\n",
    "\n",
    "#ml flow working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b212d576-0a51-4977-bf93-8b770e65b1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a3f69-4d8c-4336-af06-17f1034f4763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9d44f-d22c-4161-a5d0-6b0b1fe1dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80ec12-1536-46b4-8438-8417b95f6f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
